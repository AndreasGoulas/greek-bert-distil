# GreekBERT Distillation

This repository contains code for my diploma thesis: "Knowledge Distillation into BiLSTM Networks for the Compression of the Greek‚ÄêBERT Model".

## Requirements

* click
* numpy
* pandas
* PyTorch
* scikit-learn
* spacy (el\_core\_news\_sm)
* tqdm
* transformers

## License

The code is licensed under the [MIT License](https://opensource.org/licenses/MIT).