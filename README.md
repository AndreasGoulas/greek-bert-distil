# GreekBERT Distillation

This repository contains code for my diploma thesis: "Knowledge Distillation into BiLSTM Networks for the Compression of the Greek‚ÄêBERT Model".

## Requirements

* click
* numpy
* pandas
* PyTorch
* scikit-learn
* spacy (el\_core\_news\_sm)
* tqdm
* transformers

## License and Citation

The code is licensed under the [MIT License](https://opensource.org/licenses/MIT).

Bibtex:
```
@inproceedings{goulas2022methodology,
    author    = {Andreas Goulas and Nikolaos Malamas and Andreas L. Symeonidis},
    title     = {A Methodology for Enabling NLP Capabilities on Edge and Low-Resource Devices},
    booktitle = {Natural Language Processing and Information Systems},
    month     = {06},
    year      = {2022},
    pages     = {197--208},
    publisher = {Springer International Publishing},
    address   = {Cham}
}
```